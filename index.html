
<!DOCTYPE HTML>
<!--
	Aerial by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Autonomous Vehicle Applied Research at Nvidia</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />		
		<link rel="stylesheet" type="text/css" href="data/stylesheet.css">
		<link rel="stylesheet" href="data/academicons-1.9.1/css/academicons.css" />
		<link rel="stylesheet" href="data/fontawesome-free-6.0.0-web/css/all.css">
	</head>
	<body>
		
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>										
			<p style="text-align:center">
				<name>Autonomous Vehicles Research</name>			
			  </p>			  
			  <p style="text-align:center">
				<img src="data/nvidia.png" alt="Nvidia" style="width:100px;max-width:15%;min-width:75px">
					  <!-- <name>Nvidia</name>-->
				</p>			
			<tr style="padding:0px">
			<td style="padding:0px">				
			  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr style="padding:0px">
					<td style="padding:0.5%;width:20%;max-width:40%">					
						<img src="data/jose_alvarez_4.jpg" alt="Jose M. Alvarez" style="border-radius: 75%;margin-bottom:20px;border:2px solid #ffffff;width:300px;max-width:40%;min-width:250px">
					  </td>					  
				  <td style="width:60%;vertical-align:top">					
					<br/>
					<br>
					  <p style="text-align:left">
	  					  <font size="+2">Jose M. Alvarez</font>
					  </p>
					  I lead the Autonomous Vechicle Applied Research Group at Nvidia, CA, USA. The group focuses on scaling up deep learning for AV, spanning efficient and data-centric deep learning, 3D computer vision, and Self-Supervised Learning.
					  <br>
					<p style="text-align:left">
						<a href="https://www.linkedin.com/in/josemalvarezlopez/en" class="fab fa-fw fa-linkedin"   target="_blank"></a>						
					  <a href="mailto:jalvarez.research@gmail.com"><i class="fa fa-envelope" aria-hidden="true"   target="_blank"></i></a>&nbsp &nbsp					  					  
					  <a href="https://scholar.google.com/citations?user=Oyx-_UIAAAAJ&hl=en"   target="_blank"><i class="fas fa-fw fa-graduation-cap"></i>Scholar</a>					  					  
					</p>
				  </td>
				  
				</tr>
			  </tbody>
			</table>
		  <hr>
		  <!-- <div>
			<p style="text-align:center">
				<font size="+1" color="red">We are hiring!: <a href="https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Data-Centric-Research-Engineer---Autonomous-Vehicles_JR1955097?q=jr1955097" target="_black"><font size="+1">(senior) Research Engineers</font></a>, and <a href="https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Deep-Learning-Software-and-Research-Intern--AV-Perception_JR1949497-1?q=Deep+Learning+Software+and+Research+Intern,+AV+Perception" target="_black"><font size="+1">Software Engineer and Research interns</font></a></font>			  							
			</p>					  
		  </div>				 -->
		<!--  
		  <a href="https://icml.cc/Conferences/2022/" target="_blank"></a>		
		  
		  			  
		-->
	  	  <hr>
			<div>
				<h2><font size="+2">News</font> </h2>				
				<h2><font size="+0">2025</font> </h2>				
					<ul>			
						<li>Keynote talk at FISITA 2025, Xi'an China. Stay Tuned.</li>
						<li>2 Invited Talks at ICCV 2025 Workshops. Stay Tuned</li>
						<li>June 2025: Our <a href="https://arxiv.org/abs/2506.06664/" target="_blank">Our Generalized Trajectory Scoring Method</a> won the <a href="https://opendrivelab.com/challenge2025/" target="_blank">End-to-End Driving challenge</a> at CVPR 2025! </li> 																		
						<li>1 Paper accepted to ICCV 2025!
							<li>Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training</li>
						</li>
						<li>4 Papers accepted to CVPR 2025!
						<ul>
						<li>MDP: Multidimensional Vision Model Pruning with Latency Constraint</li>
						<li>PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models</li>
						<li>Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video</li>
						<li>OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counter Factual Reasoning</li>
						</ul>					
					</li>
					<li>We are organizing 2 tutorials at CVPR 2025.
						<ul>
						<li>Full-Stack, GPU-based Acceleration of Deep Learning tutorial.</li>
						<li><a href="https://sites.google.com/view/cvpr25-tutorial-vast" target="_blank">Advancing Data Strategies for AI success (Continuous Data Cycle via Foundation Models).</a></li>
						</ul>					
					</li>
					<li>Our paper Advancing Weight and Channel Sparsification with Enhanced Saliency has been accepted to WACV 2025!</li>
					<li>We are organizing three workshops at CVPR 2025:
						<ul>
						<li><a href="https://sites.google.com/view/nexd25/home" target="_blank">Workshop on Exploring the Next Generation of Data.</a></li>
						<li>The 8th Edition of <a href="http://cvpr2025.wad.vision/" target="_blank">Workshop on Autonomous Driving.</a></li>
						<li>The 4th Edition of <a href="https://sites.google.com/view/t4v-cvpr22/" target="_blank">Transformers for Vision (T4V workshop).</a></li>
						</ul>					
						</li>
				</ul>
				<h2><font size="+0">2024</font> </h2>				
				<ul>
					<li>September 2024: I will talk at the Green Foundation Models Workshop and the Camera based Driving workshop at ECCV 2024. </li>
					<li>September 2024: Our paper <a href="https://nvlabs.github.io/3DGM/" target="_blank">Memorize What Matters: Emergent scene decomposition from multitraverse</a> has been accepted as oral presentation to NeruIPS 2024. </li>
					<li>July 2024: Our paper <a href="https://arxiv.org/abs/2311.14671" target="_blank">SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation</a> has been accepted to ECCV 2024. </li>
					<li>June 2024: Our <a href="https://blogs.nvidia.com/blog/auto-research-cvpr-2024/" target="_blank">HydraMDP approach</a> won the <a href="https://opendrivelab.com/challenge2024/" target="_blank">End-to-End Driving at Scale challenge</a> at CVPR 2024. </li> 
					<li>June 2024: Our <a href="https://blogs.nvidia.com/blog/auto-research-cvpr-2024/" target="_blank">LLM Agent for driving </a> was the second best model in <a href="https://opendrivelab.com/challenge2024/" target="_blank">Driving With Language Challenge</a> at CVPR 2024. </li> 
					<li>March 2024: 1 paper accepted at IV 2024, and Finalist for Best Student Paper award.</li>
					<li>Feb 2024: 4 papers accepted at CVPR 2024. </li>
					<li>Feb 2024: We will host the GPU-based DL Acceleration tutorial at CVPR 2024.</li>
					<li>Feb 2024: We will host the 7th edition of the Workshop on Autonomous Driving at CVPR 2024.</li>
					<li>Jan 2024: 2 papers accepted at ICLR 2024 on efficient and robust large models.</li>
				</ul>
				<h2><font size="+0">2023</font> </h2>				
				<ul>
					<li>July 2023: 6 new papers accepted at <a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a> </li>
					<ul><li> <a href="https://github.com/NVlabs/FAN" target="_blank">Fully Attentional Networks with Self-emerging Token Labeling.</a>  Code available!</li>
						<li> <a href="https://github.com/NVlabs/FB-BEV" target="_blank">FB-BEV: BEV Representation from Forward-Backward View Transformations.</a> Code available!</li>
						<li> <a href="https://github.com/NVlabs/FocalFormer3D" target="_blank">FocalFormer3D: Focusing on Hard Instance for 3D Object Detection.</a> Code available!</li>

						<li> <a href="https://arxiv.org/abs/2307.04106" target="_blank">Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird's-Eye View.</a> Code available soon!</li>
						<li> <a href="https://nvlabs.github.io/viewpoint-robustness/" target="_blank">Towards Viewpoint Robustness in Bird's Eye View Segmentation.</a> Code and datasets available!</li>
						<li> <a href="https://github.com/NVlabs/DQTrack" target="_blank">End-to-end 3D Tracking with Decoupled Queries.</a> Code available!</li>
					</ul>					
				</ul>
				<ul>
	
					<li>April: We will be organizing a tutorial at ICCV, Paris, France 2023. Stay tunned.</li>
					<ul><li>Learning with Noisy and Unlabeled Data for Large Models beyond Categorization.</li></ul>
				</ul>
				<ul>
					<li>March: 2 new papers accepted at <a href="https://2023.ieee-iv.org/" target="_blank">IV 2023</a> </li>
					<ul><li>Shen et al. Hardware-Aware Latency Pruning for Real-Time 3D Object Detection.</li>
						<li>Clemons et al. Leaf: Legacy Networks for Flexible Inference.</li>
					</ul>									
				</ul>				
				<ul>
					<li>March: 3 new papers accepted at <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> </li>
					<ul><li> <a href="https://github.com/NVlabs/VoxFormer" target="_blank">VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion</a></li>
						<li> <a href="https://arxiv.org/abs/2205.14971" target="_blank">Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions</a></li>
						<li> <a href="https://github.com/NVlabs/mask-auto-labeler" target="_blank">Vision Transformers are Good Mask Auto-Labelers.</a> Code available!</li>
					</ul>
					<li>I will be talking about data for atonomous driving at <a href="https://www.autonomous-vehicles-conference.com/" target="_blank">Autonomous Vehicles USA 2023</a>, April 17th, Anaheim, California.</li>					
					<li>Full-Stack, GPU-based Acceleration of Deep Learning tutorial at CVPR 2023. Stay tunned.</li>
					<li>We are organizing the <a href="http://cvpr2023.wad.vision/" target="_blank">Workshop on Autonomous Driving</a> at CVPR 2023.</li>
					<li>We are organizing the <a href="https://sites.google.com/view/t4v-cvpr22/" target="_blank">T4V Workshop: Transformers for Vision</a> at CVPR 2023.</li>
				</ul>
				
				<h2><font size="+0">2022</font> </h2>				
				<ul>
					<li>Sept. 2022: 1 paper accepted at <a href="https://bmvc2022.org/" target="_blank">BMVC 2022</a>.</li>
						<ul><li>Privacy Vulnerability of Split Computing to Data-Free Model Inversion Attacks</li>
						</ul>					
					<li>September 2022: 2 papers accepted at <a href="https://nips.cc/" target="_blank">NeurIPS 2022</a></li>
					<ul><li><a href="https://arxiv.org/abs/2210.01234">Optimizing Data Collection for Machine Learning</a></li>
						<li><a href="https://halp-neurips.github.io/">Structural Pruning via Latency-Saliency Knapsack</a></li>
					</ul>
					<li>July 2022: 1 paper accepted at <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>.</li>
					<ul><li><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710640.pdf">Soft Masking for Cost-Constrained Channel Pruning</a></li>
					</ul>
					<li>April 2022: Our paper on Robustness in Transformers has been accepted at <a href="https://icml.cc/Conferences/2022/" target="_blank">ICML 2022</a></li>
					<ul><li><a href="https://arxiv.org/pdf/2204.12451.pdf" target="_blank">Understanding The Robustness in Vision Transformers</a></li>						
					</ul>
					<li>March 2022: 7 new papers accepted at <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a> </li>
					<ul><li> <a href="https://github.com/NVlabs/FreeSOLO" target="_blank">FreeSOLO: Learning to Segment Objects without Annotations</a>  </li>
						<li> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.pdf" target="_blank">How Much More Data Do I Need? Estimating Requirements for Downstream Tasks</a>  </li>
						<li> <a href="https://a-vit.github.io/" target="_blank">A-ViT: Adaptive Tokens for Efficient Vision Transformer </a></li>
						<li> <a href="https://arxiv.org/abs/2205.03783" target="_blank">Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo</a></li>
						<li> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Elezi_Not_All_Labels_Are_Equal_Rationalizing_the_Labeling_Costs_for_CVPR_2022_paper.pdf" target="_blank">Not All Labels Are Equal: Rationalizing The Labeling Costs for Training Object Detection</a> </li>
						<li> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_When_To_Prune_A_Policy_Towards_Early_Structural_Pruning_CVPR_2022_paper.pdf" target="_blank">When to Prune? A Policy towards Early Structural Pruning</a> </li>
						<li> <a href="https://github.com/zhiqi-li/Panoptic-SegFormer" target="_blank">Panoptic SegFormer: Delving deeper into panoptic segmentation with transformers</a> </li>						
					</ul>
				</ul>				
				<h2><font size="+0">2021 - Older News</font> </h2>				
				<ul>
					<li>Dec 2021: Presenting 2 papers at NeurIPS 2021: 
						<ul><li><a href="https://github.com/NVlabs/SegFormer">Segformer: Simple and efficient design for semantic segmentation with transformers</a></li>
							<li><a href="https://github.com/NVlabs/DICOD">Distilling Image Classifiers in Object Detectors</a></li>
						</ul>
					</li>			
					<li>October 2021: Presenting our active learning paper at ICCV 2021
						<ul><li><a href="https://github.com/NVlabs/AL-MDN">Active learning for deep object detection via probabilistic modeling</a></li>
						</ul>
						</li>			 		
					<li>1 new paper accepted at ICML 2021</li>
					<ul><li><a href="https://arxiv.org/abs/2104.05702">Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection</a></li>
						</ul>
					<li>2 new papers accepted at ICLR 2021</li>
					<li>1 new paper accepted at ICCV 2021</li>
					<li>3 new papers accepted at CVPR 2021</li>
					<li>1 new paper accepted at WACV 2021</li>
					<li>1 new paper accepted at IEEE-IV 2021</li>
					<li>2 new papers accepted at IEEE CVPR 2020</li>
					<li>1 new paper accepted at IEEE-IV 2020</li>
					<li>1 new paper accepted at NeurIPS 2020</li>
					<li>1 new paper accepted at WACV 2020</li>
					<li>1 new paper accepted at ICLR 2020</li>
				</ul>
			</div>		
		<hr>
			<div>
				<h2><font size="+1">Interns (2024)</font> </h2>		
				<ul><li>Zhenxin Li – MSc, Fudan Univ.</li>					
					<li>Shihao Wang – MSc, Beijing Institute of Technology</li>
					<li>Zi Wang – MSc, Carnegie Mellon Univ.</li>
					<li>Sihan Liu – MSc, Fudan Univ.</li>
					<li>Shan Wang – PhD, Australian National Univ.</li>
					<li>Nikita Durasov – PhD, EPFL</li>
					<li>Jenny Schmalfuss – PhD, University of Stuttgart</li>
					<li>Chonghao Sima – PhD, Hong Kong Univ.</li>
					<li>Jihao Liu – PhD, The Chinese University of Hong Kong.</li>
					<li>Feiyang Kang – PhD, Virginia Tech Univ.</li>
					<li>Vibashan VS – PhD, John Hopkins University</li>
				</ul>
				<h2><font size="+1">Interns (2023)</font> </h2>	
				<ul><li>Zhiqi Li – PhD, Nanjing Univ.</li>
					<li>Yiming Li – PhD, NYU</li>					
				</ul>
				<h2><font size="+1">Interns (2022)</font> </h2>
				<ul><li>David Wang – PhD, Tsinghua Univ.</li>					
					<li>Shixing Yu – PhD, Univ. Texas Austin</li>					
					<li>Yanwei Li – PhD, CUHK</li>					
					<li>Zhiqi Li – PhD, Nanjing Univ.</li>					
					<li>Zetong Yang – PhD, CUHK</li>					
					<li>Tzofi Klinghoffer – PhD, MIT</li>					
					<li>Bingyin Zhao – PhD, Clemson Univ.</li>					
					<li>Annamarie Bair – PhD, CMU</li>					
					<li>Yilun Chen – PhD, CUHK</li>					
					<li>Alex Sun  – MS, Stanford</li>					
					<li>Justin Hsu – BS, Stanford</li>					
					<li>Arnav Joshi – BS, Stanford</li>					
					<li>Shubhranshu Sigh – MS, CMU</li>					
				</ul>
				<h2><font size="+1">Interns (2021)</font> </h2>
				<ul><li>Jiayu Yang – PhD, Australian National University</li>					
					<li>Marc Finzi – PhD, NYU</li>					
					<li>Enze Xie – PhD, HKU</li>					
					<li>Ryan Humble – PhD, Stanford</li>					
					<li>Vlad Sobal – PhD, NYU</li>					
					<li>Jianna Liu – BS, MIT</li>					
					<li>Xinlong Wang – PhD, Univ. Adelaide</li>					
					<li>Daquan Zhou – PhD, NUS</li>					
					<li>Faith Johnson – PhD, Rutgers</li>					
					<li>Javier Sagastuy Brena – PhD, Stanford</li>					
					<li>Joshua Chen – MS, CMU (now Nvidia)</li>					
					<li>Nithya Attaluri – BS, MIT</li>					
					<li>Jessica Lee – MS, CMU</li>					
				</ul>
				<h2><font size="+1">Interns (2020)</font> </h2>
				<ul><li>Jiwoong Choi PhD, Seoul Nat. University (now NVIDIA)</li>					
					<li>Ismail Elezi PhD TUM (now PostDoc TUM)</li>					
					<li>Shuxuan Guo - PhD. EPFL</li>					
					<li>Marvin Kim – MS CMU (now Waymo)</li>					
					<li>Lilian Luong – BS MIT</li>					
					<li>Cynthia Liu – MS, MIT (now Cerebras)</li>	
					<li>Nadine Chang – PhD CMU	</li>
					<li>Yerlan Idelbayav – PhD UC Merced (now Amazon)</li>					
					<li>Xinnan Du – MS CMU (now Goldman Sachs)</li>					
					<li>Hoang Vu Nguyen – PhD Stony Brooks Univ.</li>					
				</ul>
				<h2><font size="+1">Interns (2019)</font> </h2>
				<ul><li>Maying Shen – MS CMU (now Nvidia)</li>					
					<li>Michael Zhang – BS Hardvard (now PhD Stanford)</li>					
					<li>Akshay Chawla – MS CMU (now Vicarious)</li>					
					<li>Kashyap Chitta – MS CMU (now PhD student MPI)</li>					
					<li>Tony Wang – BS MIT</li>					
					<li>Wenbo Guo – PhD Penn. State Univ.</li>
					<li>Nikhil Murthy – BS MIT</li>									
				</ul>
				<h2><font size="+1">Interns (2018)</font> </h2>
				<ul><li>Jiaming Zeng – PhD Stanford (now IBM Research)</li>					
					<li>Yousef Hindy – MS Stanford (now NewCo)</li>							
					<li>Kashyap Chitta – MS CMU (now PhD student MPI)</li>								
				</ul>								
			</div>
			<hr>
			<div>
				<h2><font size="+1">Invited Talks</font> </h2>
				<ul>					
					<li>[Nadine Chang] Less is More: Accelerating AI with Advanced Data Strategies, August 2025 Toronto, Canada.</li>
					<li>[Nadine Chang] Less is More: Accelerating AI with Advanced Data Strategies, July 2025 Barcelona, Spain.</li>
					<li>Towards Safer and More Generalizable Autonomous Vehicles, FISITA 2025, Xi'an China</li>
					<li>Multimodal Planners for Dynamic Environments, Interaction-driven Behavior, Prediction and Planning for Autonomous Vehicles Workshop, IEEE-IV 2025</li>
					<li>Towards Safer and More Generalizable Autonomous Vehicles, CVPR Safe Arificial Intelligence Workshop 2025, Nashville , USA</li>
					<li>From Research to Product: Transforming AV Technology with AI, GTC 2025, San Jose, California, USA</li>
					<li>Towards Robust and Reliable Autonomous Vehicles with Foundation Models, NYU Seminar, Oct 2024, New York, USA</li>					
					<li>Towards Safe and Reliable Autonomous Vehicles, CMU, Pittsburgh, USA</li>
					<li>Towards an Effective use of Foundation Models in Autonomous Driving, Geen Foundation Model workshop @ECCV 2024, Milan, Italy</li>
					<li>Towards Robust and Reliable AV with Foundational Models, 2nd Workshop on Vision-Centric Autonomous Driving @ ECCV 2024, Milan, Italy</li>					
					<li>Camera-based perception for AV: From Data Collection to network Robustness, E2E Autonomous Driving Workshop @ CVPR 2023</li>
					<li>Optimizing large deep models for real-time inference, Embedded Vision Workshop @ CVPR 2023</li>
					<li>Achieving Better Accuracy in 3D Occupancy Prediction for Autonomous Driving, GTC 2024, San Jose, California, USA</li>
					<li>Efficient Deep Learning at Scale, UvA Seminar, October 2022, Amsterdam, The Netherlands</li>
					<li>Towards Robust Perception Systems in Open World, GTC 2022, San Jose, California, USA</li>
					<li>Deep Learning at Scale, UvA Seminar, October 2021, Amsterdam, The Netherlands</li>
					<li>Scaling-up Deep Learning for Autonomous Driving, GTC 2019, San Jose, California, USA</li>
					<li>Scaling-up Deep Learning for Autonomous Driving, MVA'19 Invited Tutorial, Tokyo, Japan</li>
					<li>Scaling up Deep Learning for Autonomous Driving, ECCV 2018 workshop, Munich, Germany</li>
					<li>Efficient ConvNets for Perception in Autonomous Driving, ICML'2017, Sydney, Australia</li>
					<li>Efficient Deep Model Selection. GTC'17, San Jose, CA, US</li>
					<li>Efficient Networks for Real Time Scene Understanding. GTCx Australia'16. Melbourne, Australia</li>
					<li>Large-scale scene understanding in constrained platforms. NVIDIA RoadShow. Canberra, Australia</li>
					<li>Compacting ConvNets for end to end learning. Deep Learning workshop ACRA 2015</li>					
				</ul> 
			</div>
			<hr>
			<div>
				<h2><font size="+1">Academic Service</font> </h2>
				<ul>
					<li>I served as an Area Chair for NeurIPS 2025-2024, IJCAI (2025; 2022), KDD 2024, ECCV 2024, IV 2018-2025, IEEE ITSC 2018, MM 2017, IEEE ICRA 2015, IEEE WACV 2016-2018</li>
					<li>I am Associate Editor for IEEE TPAMI</li>
					<li>I am Associate Editor for IEEE T-ITS</li>					
					<li>I was a Chair IEEE-ACT Comp. Science Chapter</li>					
				</ul>
			</div>						
			<hr>
			<div>
				<h2><font size="+1">Awards and Recognitions</font> </h2>
				<ul><li> <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">SegFormer Ranked 3 in NeurIPS 2021 Top-10 Influential Papers.</a></li>
					<li> <a href="https://ieee-itss.org/pub/t-its/t-its-paper-award-history/#:~:text=The%20George%20N.%20Saridis%20Best%20Transactions%20Paper%20Award,calendar%20years%20preceding%20the%20year%20of%20the%20award.">2020 Outstanding research for "ErfNet: Efficient Residual Factorized ConvNEt for Real-Time Semantic Segmentaiton".</a></li>								
					<li> <a href="https://ieee-itss.org/pub/t-its/t-its-paper-award-history/#:~:text=The%20George%20N.%20Saridis%20Best%20Transactions%20Paper%20Award,calendar%20years%20preceding%20the%20year%20of%20the%20award".>Road Detection based on Illuminance-Invariance awarded: 2010-2019 Top Ten Research Papers </a></li>
					<li> Best Paper award for Efficient ConvNet for Real-time Semantic Segmentation at IV 2017</li>
					<li> <a href="https://iccv2021.thecvf.com/outstanding-reviewers">Outstanding Reviewer Awards at ICCV 2021</a></li>
					<li> <a href="https://nips.cc/Conferences/2019/Reviewers">Awarded 2019 Top Reviewers at NeurIPS 2019</a></li>
					<li> <a href="https://cvpr2017.thecvf.com/files/OutstandingReviewers.pdf">Outstanding Reviewer Awards at CVPR 2017</a></li>
					<li> <a href="http://adas.cvc.uab.es/CVVT2013/index.html#awards">Best Paper Award for Exploiting Sparsity for Real Time Video Labelling, at CVPR workshop on Computer Vision for Vehicle Technology  2013</a></li>
					<li> Best Poster Award for Synchronization of video sequences from free-moving cameras, at IbPria  2007</li>
					<li> Ramon y Cajal Fellowship, Spain, 2016</li>
					<li> ERC - Fellow Grant, 2011</li>
					<li> Extraordinary Doctorate award, Barcelona, Spain, 2010-2011</li>
					<li> Best PhD award 2010, Barcelona, Spain</li>
				</ul>
			</div>
			<hr>
			<div>								
				<h2><font size="+1">Relevant Workshop Organization</font> </h2>
				<ul>
                    <li><a href="https://sites.google.com/view/t4v-cvpr22">Transformers for Vision at CVPR 2022</a></li>
					<li>Deep-Vision Workshop at CVPR, 2014 to 2020</li>									
					<li><a href="https://sites.google.com/view/vehicleperceptionatscale">Tutorial on Scaling up Deep Learning for Autonomous Driving at IV 2020</a></li>	
					<li><a href="https://sites.google.com/view/iv2022-bsl-workshop">Beyond Supervised Learning at IV 2022</a></li>						
					<li><a href="https://embodied-ai.org">Embodied AI Workshop at CVPR</a></li>
					<li>Workshop on Autonomous Driving at CVPR, 2019 to 2022</li>				
					<li>Color in Computer Vision at ECCV / ICCV 2013 to 2017</li>				
					<li>Computer Vision for Road Scene Understanding at ECCV / ICCV 2014 to 2018</li>
					<li>Computer Vision in Vehicle Technology, at CVPR 2015 to 2018</li>
					<li>Deep-Driving at IV 2014 to 2017</li>						
				</ul> 

			</div>
		</td>
	</tr>
</table>

<!-- Default Statcounter code for Jose M. Alvarez research
https://alvarezlopezjosem.github.io/ -->
<script type="text/javascript">
	var sc_project=6053472; 
	var sc_invisible=1; 
	var sc_security="3500293f"; 
	</script>
	<script type="text/javascript"
	src="https://www.statcounter.com/counter/counter.js"
	async></script>
	<noscript><div class="statcounter"><a title="web statistics"
	href="https://statcounter.com/" target="_blank"><img
	class="statcounter"
	src="https://c.statcounter.com/6053472/0/3500293f/1/"
	alt="web statistics"
	referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
	<!-- End of Statcounter Code -->
</body>  			  		  
</html>